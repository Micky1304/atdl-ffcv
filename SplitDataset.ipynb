{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e4dc583-7310-4878-9798-4b72cf247009",
   "metadata": {},
   "source": [
    "# Splitting the large image to many smaller ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04fd4b7b-500f-4627-ab86-5791a2752b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7e43810-1485-4c0e-8ca6-66056f96a2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23074/3320883609.py:3: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  tif_image = imageio.imread(tif_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57832, 94303, 3)\n"
     ]
    }
   ],
   "source": [
    "#As FFCV needs each instance to fit on a single page, the sizes are reduced\n",
    "tif_path = \"data/train_area.tif\"\n",
    "tif_image = imageio.imread(tif_path)\n",
    "data = np.array(tif_image)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f04df408-f161-44cb-9eaf-572536dbade6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 47 2256\n"
     ]
    }
   ],
   "source": [
    "height = 57832 // 1200\n",
    "width = 94303 // 2000\n",
    "print(height, width, height*width)\n",
    "#2256 Files will be created with shape (1200, 2000, 3)\n",
    "#Later each batch contains 48 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "290cd4c9-5b57-4dee-b118-86ddc0993c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Height part 1/48 done.\n",
      "Height part 2/48 done.\n",
      "Height part 3/48 done.\n",
      "Height part 4/48 done.\n",
      "Height part 5/48 done.\n",
      "Height part 6/48 done.\n",
      "Height part 7/48 done.\n",
      "Height part 8/48 done.\n",
      "Height part 9/48 done.\n",
      "Height part 10/48 done.\n",
      "Height part 11/48 done.\n",
      "Height part 12/48 done.\n",
      "Height part 13/48 done.\n",
      "Height part 14/48 done.\n",
      "Height part 15/48 done.\n",
      "Height part 16/48 done.\n",
      "Height part 17/48 done.\n",
      "Height part 18/48 done.\n",
      "Height part 19/48 done.\n",
      "Height part 20/48 done.\n",
      "Height part 21/48 done.\n",
      "Height part 22/48 done.\n",
      "Height part 23/48 done.\n",
      "Height part 24/48 done.\n",
      "Height part 25/48 done.\n",
      "Height part 26/48 done.\n",
      "Height part 27/48 done.\n",
      "Height part 28/48 done.\n",
      "Height part 29/48 done.\n",
      "Height part 30/48 done.\n",
      "Height part 31/48 done.\n",
      "Height part 32/48 done.\n",
      "Height part 33/48 done.\n",
      "Height part 34/48 done.\n",
      "Height part 35/48 done.\n",
      "Height part 36/48 done.\n",
      "Height part 37/48 done.\n",
      "Height part 38/48 done.\n",
      "Height part 39/48 done.\n",
      "Height part 40/48 done.\n",
      "Height part 41/48 done.\n",
      "Height part 42/48 done.\n",
      "Height part 43/48 done.\n",
      "Height part 44/48 done.\n",
      "Height part 45/48 done.\n",
      "Height part 46/48 done.\n",
      "Height part 47/48 done.\n",
      "Height part 48/48 done.\n"
     ]
    }
   ],
   "source": [
    "folderpath = \"data/train_small_npy\"\n",
    "height = 1200\n",
    "width = 2000\n",
    "\n",
    "height_parts = data.shape[0] // height\n",
    "width_parts = data.shape[1] // width\n",
    "\n",
    "for i in range(height_parts):\n",
    "    for j in range(width_parts):\n",
    "        part = data[i * height: (i + 1) * height, j * width: (j + 1) * width]\n",
    "        \n",
    "        #Convert the shape of each ndarray to (3, 1200, 2000) for processing in PyTorch\n",
    "        part = np.transpose(part, (2, 0, 1))\n",
    "        \n",
    "        filepath = os.path.join(folderpath, f\"part_{i}_{j}.npy\")\n",
    "        np.save(filepath, part)\n",
    "    print(f\"Height part {i+1}/{height_parts} done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c109210-afa9-4334-b2e9-c1d64be211f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2257\n"
     ]
    }
   ],
   "source": [
    "# Label dictionary already included\n",
    "print(len(os.listdir(folderpath)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da08749-89e3-4e5c-a939-a82aa8d0c241",
   "metadata": {},
   "source": [
    "# Add dictionary for labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f9ab538-6067-4ccf-9204-71c29aeb089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = os.listdir(folderpath)\n",
    "np.random.seed(seed=None)\n",
    "\n",
    "labels_dict = {}\n",
    "\n",
    "for filename in file_list:\n",
    "    if filename.startswith(\"part_\"):\n",
    "        label = np.random.uniform(0, 100)\n",
    "        labels_dict[filename] = label\n",
    "\n",
    "np.save(os.path.join(folderpath, \"labels.npy\"), labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64296307-5f47-4688-bf8d-577c6daded58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2256\n"
     ]
    }
   ],
   "source": [
    "print(len(labels_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904c2e64-c667-49c5-8b18-2729fa545d9b",
   "metadata": {},
   "source": [
    "# Creating the .beton file for FFCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fb67c41-8208-4329-afef-af4084cf07e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: The TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.\n",
      "  warnings.warn(problem)\n"
     ]
    }
   ],
   "source": [
    "!export NUMBA_THREADING_LAYER='omp'\n",
    "import ffcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29534d39-5b1e-4dfe-8dd8-9dac24959f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ffcv.writer import DatasetWriter\n",
    "from ffcv.fields import NDArrayField, FloatField\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89569dc6-685d-430b-b840-a1c9d549d571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identical definition as in PyTorch\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.file_list = [f for f in os.listdir(data_dir) if f.startswith(\"part_\")]\n",
    "        self.labels = np.load(os.path.join(data_dir, \"labels.npy\"), allow_pickle = True).item()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.data_dir, self.file_list[idx])\n",
    "        image = np.load(file_path)\n",
    "        label = self.labels[self.file_list[idx]]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5896fbff-e949-430f-b859-089b9d46a5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/train_small_npy\"\n",
    "\n",
    "dataset = CustomDataset(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26d3f3a3-2f0d-40ca-8d32-8a4bffae4278",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = DatasetWriter(\"data/train_data.beton\", {\n",
    "    \"image\": NDArrayField(shape=(3, 1200, 2000), dtype = np.dtype(\"uint8\")),\n",
    "    \"label\": FloatField()\n",
    "}, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "214184c1-8632-4383-8e80-eb57bcc89ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2256/2256 [00:14<00:00, 159.46it/s]\n"
     ]
    }
   ],
   "source": [
    "writer.from_indexed_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7016b58f-40bd-4bea-bc5a-614994e978af",
   "metadata": {},
   "source": [
    "# Copy the data to ssd storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e64fc4f-d7c6-416b-a438-3078010ba076",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Command line commands\n",
    "cp -r saved_data/data/train_small_npy ssd\n",
    "cp saved_data/data/train_data.beton ssd\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8003730f-f15b-49e7-8d70-d759ed5d59a7",
   "metadata": {},
   "source": [
    "# Generate 200 GB dataset to exceed RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf60ba15-ee69-4778-be85-bfb64688ae53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2257\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(len(os.listdir(\"data/train_small_npy\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "057ece74-8fba-4285-8fe1-0534c72402f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2256\n",
      "(3, 1200, 2000)\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data/train_small_npy\"\n",
    "file_list = [f for f in os.listdir(data_dir) if f.startswith(\"part_\")]\n",
    "print(len(file_list))\n",
    "data = np.load('data/train_small_npy/part_45_12.npy')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5eb735c3-15bd-49b9-b15a-f50e04b133fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1/12 Done.\n",
      "Progress: 2/12 Done.\n",
      "Progress: 3/12 Done.\n",
      "Progress: 4/12 Done.\n",
      "Progress: 5/12 Done.\n",
      "Progress: 6/12 Done.\n",
      "Progress: 7/12 Done.\n",
      "Progress: 8/12 Done.\n",
      "Progress: 9/12 Done.\n",
      "Progress: 10/12 Done.\n",
      "Progress: 11/12 Done.\n",
      "Progress: 12/12 Done.\n"
     ]
    }
   ],
   "source": [
    "# Format: part_heigth_width_left/right_copy\n",
    "# with heigth 0-47, width 0-46, left/right 0-1, copy 0-12\n",
    "data_dir = \"data/train_small_npy\"\n",
    "new_data_dir = \"data/train_200GB\"\n",
    "counter = 0\n",
    "for file in file_list:\n",
    "    counter += 1\n",
    "    \n",
    "    filename = file[:-4]\n",
    "    image = np.load(os.path.join(data_dir, file))\n",
    "    left_half = image[:, :, :1000]\n",
    "    right_half = image[:, :, 1000:]\n",
    "    for copy in range(13):\n",
    "        left_name = f\"{filename}_0_{copy}.npy\" \n",
    "        right_name = f\"{filename}_1_{copy}.npy\"\n",
    "        np.save(os.path.join(new_data_dir, left_name), left_half)\n",
    "        np.save(os.path.join(new_data_dir, right_name), left_half)\n",
    "    if (counter%188 == 0):\n",
    "        print(f\"Progress: {int(counter/188)}/12 Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b07e6c40-3783-4461-a2a9-a0a0a28ff7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries in dict: 58656\n"
     ]
    }
   ],
   "source": [
    "# Addition of labels.npy \n",
    "new_data_dir = \"data/train_200GB\"\n",
    "file_list = [f for f in os.listdir(new_data_dir) if f.startswith(\"part_\")]\n",
    "np.random.seed(seed=1)\n",
    "\n",
    "labels_dict = {}\n",
    "\n",
    "for filename in file_list:\n",
    "    if filename.startswith(\"part_\"):\n",
    "        label = np.random.uniform(0, 100)\n",
    "        labels_dict[filename] = label\n",
    "\n",
    "# Expected entries: 2256 * 2 * 13 = 58656 \n",
    "print(\"Entries in dict:\", len(labels_dict))\n",
    "np.save(os.path.join(new_data_dir, \"labels.npy\"), labels_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c745415d-e801-48f0-9997-e82da6e9c611",
   "metadata": {},
   "source": [
    "# Conversion of 200GB dataset to .beton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "747a34c1-dfbf-4b76-843d-d08887140b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: The TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.\n",
      "  warnings.warn(problem)\n"
     ]
    }
   ],
   "source": [
    "!export NUMBA_THREADING_LAYER='omp'\n",
    "import ffcv\n",
    "from ffcv.writer import DatasetWriter\n",
    "from ffcv.fields import NDArrayField, FloatField\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "88ebd25e-868d-4c7c-a57d-28e3faa8732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.file_list = [f for f in os.listdir(data_dir) if f.startswith(\"part_\")]\n",
    "        self.labels = np.load(os.path.join(data_dir, \"labels.npy\"), allow_pickle = True).item()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.data_dir, self.file_list[idx])\n",
    "        image = np.load(file_path)\n",
    "        label = self.labels[self.file_list[idx]]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff80023a-e5b6-412b-9558-89a1742dcf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_dir = \"data/train_200GB\"\n",
    "\n",
    "dataset = CustomDataset(new_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed90cb6d-3b93-4d42-b52e-81bfbef3e3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = DatasetWriter(\"data/train_200GB.beton\", {\n",
    "    \"image\": NDArrayField(shape=(3, 1200, 1000), dtype = np.dtype(\"uint8\")),\n",
    "    \"label\": FloatField()\n",
    "}, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b5eb7ba8-167c-4cee-9555-d979cc73460f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58656/58656 [22:23<00:00, 43.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 1343.9033253192902\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "writer.from_indexed_dataset(dataset)\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time-start_time}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
