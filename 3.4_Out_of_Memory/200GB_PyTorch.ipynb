{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3916d99-d05f-442b-8b05-f06bf6c84fb6",
   "metadata": {},
   "source": [
    "# Loading the 200GB dataset with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24d70b37-695e-4ad6-b1ee-53ac1435b09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.file_list = [f for f in os.listdir(data_dir) if f.startswith(\"part_\")]\n",
    "        self.labels = np.load(os.path.join(data_dir, \"labels.npy\"), allow_pickle = True).item()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.data_dir, self.file_list[idx])\n",
    "        image = np.load(file_path)\n",
    "        label = self.labels[self.file_list[idx]]\n",
    "        return image, label\n",
    "\n",
    "data_dir = \"data/train_200GB\"\n",
    "dataset = CustomDataset(data_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=48, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9669a04e-3d86-4c69-867a-969c547532de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 done in 519.0264642238617 seconds.\n",
      "Epoch 2 done in 527.9110577106476 seconds.\n",
      "Epoch 3 done in 564.508202791214 seconds.\n",
      "Epoch 4 done in 504.62781500816345 seconds.\n",
      "Epoch 5 done in 535.0007140636444 seconds.\n",
      "Epoch 6 done in 585.1651082038879 seconds.\n",
      "Epoch 7 done in 531.1468064785004 seconds.\n",
      "Epoch 8 done in 642.8812713623047 seconds.\n",
      "Epoch 9 done in 563.2509710788727 seconds.\n",
      "Epoch 10 done in 572.5111229419708 seconds.\n",
      "Epoch 11 done in 641.9526226520538 seconds.\n",
      "Epoch 12 done in 557.3534407615662 seconds.\n",
      "Epoch 13 done in 614.1739091873169 seconds.\n",
      "Epoch 14 done in 612.5322651863098 seconds.\n",
      "Epoch 15 done in 556.8544085025787 seconds.\n",
      "Epoch 16 done in 591.4019739627838 seconds.\n",
      "Epoch 17 done in 529.8777468204498 seconds.\n",
      "Epoch 18 done in 545.0977780818939 seconds.\n",
      "Epoch 19 done in 531.2759389877319 seconds.\n",
      "Epoch 20 done in 593.2343735694885 seconds.\n",
      "Total time taken: 11319.82952952385 seconds\n",
      "Total time per epoch: 565.9914764761925 seconds\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "    for images, labels in dataloader:\n",
    "        pass\n",
    "    epoch_end = time.time()\n",
    "    epoch_time = epoch_end - epoch_start\n",
    "    print(f\"Epoch {epoch+1} done in {epoch_time} seconds.\")\n",
    "end_time = time.time()\n",
    "\n",
    "total_time = end_time - start_time\n",
    "\n",
    "print(f\"Total time taken: {total_time} seconds\")\n",
    "print(f\"Total time per epoch: {total_time/num_epochs} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222edb32-9377-4633-a440-395fe8bef138",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
